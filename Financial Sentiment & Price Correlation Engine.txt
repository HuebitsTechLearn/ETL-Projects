import requests
import pandas as pd
import datetime
import json
import os
import yfinance as yf
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# SQLAlchemy for database interaction
from sqlalchemy import create_engine, Column, String, Integer, DateTime, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# Matplotlib and Seaborn for visualization
import matplotlib.pyplot as plt
import seaborn as sns

# --- NLTK Data Download (Run once) ---
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except nltk.downloader.DownloadError:
    print("Downloading NLTK vader_lexicon for sentiment analysis...")
    nltk.download('vader_lexicon')
    print("NLTK vader_lexicon downloaded.")

# --- Configuration ---
# IMPORTANT: Replace with your actual NewsAPI Key
# You can get one by signing up at https://newsapi.org/
NEWS_API_KEY = "YOUR_NEWS_API_KEY"
NEWS_API_BASE_URL = "https://newsapi.org/v2/everything"

# Stock symbols to track (e.g., Apple, Microsoft, Google)
STOCK_SYMBOLS = ["AAPL", "MSFT", "GOOGL"]

# Database path
DATABASE_URL = "sqlite:///./financial_sentiment_analytics.db" # SQLite database file

# --- Database Setup (SQLAlchemy) ---
engine = create_engine(DATABASE_URL)
Base = declarative_base()

# Define the FinancialNewsSentiment model
class FinancialNewsSentiment(Base):
    __tablename__ = "financial_news_sentiment"
    id = Column(Integer, primary_key=True, autoincrement=True)
    symbol = Column(String, nullable=False, index=True) # Stock symbol related to the news
    published_at = Column(DateTime, nullable=False, index=True) # UTC timestamp of publication
    source_name = Column(String)
    title = Column(String)
    description = Column(String)
    url = Column(String)
    sentiment_compound = Column(Float)
    sentiment_pos = Column(Float)
    sentiment_neg = Column(Float)
    sentiment_neu = Column(Float)

    def __repr__(self):
        return (f"<FinancialNewsSentiment(symbol='{self.symbol}', title='{self.title[:30]}...', "
                f"sentiment={self.sentiment_compound:.2f}, published='{self.published_at.strftime('%Y-%m-%d %H:%M')}')>")

# Define the StockPriceData model
class StockPriceData(Base):
    __tablename__ = "stock_price_data"
    id = Column(Integer, primary_key=True, autoincrement=True)
    symbol = Column(String, nullable=False, index=True)
    date = Column(DateTime, nullable=False, index=True) # Date of the price record
    open_price = Column(Float)
    high_price = Column(Float)
    low_price = Column(Float)
    close_price = Column(Float)
    volume = Column(Integer)

    def __repr__(self):
        return (f"<StockPriceData(symbol='{self.symbol}', date='{self.date.strftime('%Y-%m-%d')}', "
                f"close={self.close_price:.2f})>")

# Create tables in the database if they don't exist
Base.metadata.create_all(engine)

# Create a session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# --- Data Extraction ---
def extract_financial_news(api_key: str, query: str, from_date: datetime.date, to_date: datetime.date):
    """
    Fetches financial news headlines from NewsAPI for a given query and date range.
    NewsAPI free tier allows max 1 month historical data.
    """
    print(f"Fetching news for '{query}' from {from_date} to {to_date}...")
    all_articles = []
    params = {
        "q": query,
        "apiKey": api_key,
        "language": "en",
        "sortBy": "relevancy",
        "from": from_date.isoformat(),
        "to": to_date.isoformat(),
        "pageSize": 100 # Max page size
    }
    try:
        response = requests.get(NEWS_API_BASE_URL, params=params)
        response.raise_for_status()
        data = response.json()
        if data.get("status") == "ok":
            print(f"Found {data.get('totalResults')} articles for '{query}'.")
            all_articles.extend(data.get("articles", []))
        else:
            print(f"NewsAPI error for '{query}': {data.get('message')}")
    except requests.exceptions.RequestException as e:
        print(f"Error fetching news for '{query}': {e}")
    except json.JSONDecodeError:
        print(f"Failed to decode JSON response from NewsAPI for '{query}'.")
    return all_articles

def extract_stock_prices(symbol: str, start_date: datetime.date, end_date: datetime.date):
    """
    Fetches historical stock prices using yFinance.
    """
    print(f"Fetching stock prices for {symbol} from {start_date} to {end_date}...")
    try:
        # yfinance expects date strings in 'YYYY-MM-DD' format
        df = yf.download(symbol, start=start_date.isoformat(), end=end_date.isoformat(), progress=False)
        if df.empty:
            print(f"No stock data found for {symbol} in the specified range.")
            return None
        df = df.reset_index() # Convert index to a column
        df.rename(columns={'Date': 'date', 'Open': 'open_price', 'High': 'high_price',
                           'Low': 'low_price', 'Close': 'close_price', 'Volume': 'volume'}, inplace=True)
        df['symbol'] = symbol
        print(f"Fetched {len(df)} price records for {symbol}.")
        return df[['symbol', 'date', 'open_price', 'high_price', 'low_price', 'close_price', 'volume']]
    except Exception as e:
        print(f"Error fetching stock prices for {symbol}: {e}")
        return None

# --- Sentiment Analysis ---
def analyze_sentiment(text: str):
    """
    Performs sentiment analysis using NLTK's VaderSentiment.
    Returns compound, positive, negative, and neutral scores.
    """
    if not text:
        return {'compound': 0.0, 'pos': 0.0, 'neg': 0.0, 'neu': 0.0}
    scores = analyzer.polarity_scores(text)
    return scores

# --- Data Transformation ---
def transform_news_data(raw_news_list: list, symbol: str):
    """
    Transforms raw news articles into a DataFrame with sentiment scores.
    """
    if not raw_news_list:
        return pd.DataFrame()

    df_news = pd.DataFrame(raw_news_list)
    df_news['symbol'] = symbol
    df_news['published_at'] = pd.to_datetime(df_news['publishedAt']).dt.tz_localize(None) # Convert to datetime and remove timezone
    df_news['source_name'] = df_news['source'].apply(lambda x: x.get('name'))

    # Apply sentiment analysis to title and description (concatenated)
    df_news['full_text'] = df_news['title'].fillna('') + " " + df_news['description'].fillna('')
    sentiment_scores = df_news['full_text'].apply(analyze_sentiment)
    df_news = pd.concat([df_news, pd.json_normalize(sentiment_scores)], axis=1)

    # Select and reorder columns for the database model
    transformed_df = df_news[[
        'symbol', 'published_at', 'source_name', 'title', 'description', 'url',
        'compound', 'pos', 'neg', 'neu'
    ]].rename(columns={
        'compound': 'sentiment_compound',
        'pos': 'sentiment_pos',
        'neg': 'sentiment_neg',
        'neu': 'sentiment_neu'
    })
    print(f"Transformed {len(transformed_df)} news records for {symbol}.")
    return transformed_df

# --- Data Loading ---
def load_news_sentiment_to_db(df: pd.DataFrame, db_session_factory):
    """
    Loads the DataFrame of news sentiment into the database.
    """
    if df.empty:
        print("No news sentiment data to load.")
        return

    print(f"Loading {len(df)} news sentiment records into the database...")
    with db_session_factory() as db:
        for index, row in df.iterrows():
            # Check for existing entry to avoid duplicates if running multiple times
            # A more robust check might involve hashing title+published_at
            existing_entry = db.query(FinancialNewsSentiment).filter_by(
                symbol=row['symbol'],
                published_at=row['published_at'],
                title=row['title']
            ).first()
            if not existing_entry:
                news_entry = FinancialNewsSentiment(
                    symbol=row['symbol'],
                    published_at=row['published_at'],
                    source_name=row['source_name'],
                    title=row['title'],
                    description=row['description'],
                    url=row['url'],
                    sentiment_compound=row['sentiment_compound'],
                    sentiment_pos=row['sentiment_pos'],
                    sentiment_neg=row['sentiment_neg'],
                    sentiment_neu=row['sentiment_neu']
                )
                db.add(news_entry)
        db.commit()
    print("News sentiment data loaded successfully.")

def load_stock_prices_to_db(df: pd.DataFrame, db_session_factory):
    """
    Loads the DataFrame of stock prices into the database.
    """
    if df.empty:
        print("No stock price data to load.")
        return

    print(f"Loading {len(df)} stock price records into the database...")
    with db_session_factory() as db:
        for index, row in df.iterrows():
            # Check for existing entry to avoid duplicates
            existing_entry = db.query(StockPriceData).filter_by(
                symbol=row['symbol'],
                date=row['date']
            ).first()
            if not existing_entry:
                price_entry = StockPriceData(
                    symbol=row['symbol'],
                    date=row['date'],
                    open_price=row['open_price'],
                    high_price=row['high_price'],
                    low_price=row['low_price'],
                    close_price=row['close_price'],
                    volume=row['volume']
                )
                db.add(price_entry)
        db.commit()
    print("Stock price data loaded successfully.")

# --- Correlation Analysis ---
def perform_correlation_analysis(db_session_factory):
    """
    Fetches sentiment and price data, joins them, and calculates correlations.
    """
    print("\n--- Performing Correlation Analysis ---")

    with db_session_factory() as db:
        # Fetch all news sentiment data
        news_records = db.query(FinancialNewsSentiment).all()
        df_news_raw = pd.DataFrame([{
            'symbol': rec.symbol,
            'published_at': rec.published_at,
            'sentiment_compound': rec.sentiment_compound
        } for rec in news_records])

        # Fetch all stock price data
        price_records = db.query(StockPriceData).all()
        df_prices_raw = pd.DataFrame([{
            'symbol': rec.symbol,
            'date': rec.date,
            'close_price': rec.close_price
        } for rec in price_records])

    if df_news_raw.empty or df_prices_raw.empty:
        print("Not enough data for correlation analysis.")
        return

    # Aggregate news sentiment by day and symbol
    df_news_raw['date'] = df_news_raw['published_at'].dt.date
    daily_sentiment = df_news_raw.groupby(['symbol', 'date'])['sentiment_compound'].mean().reset_index()
    daily_sentiment.rename(columns={'sentiment_compound': 'avg_daily_sentiment'}, inplace=True)
    daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date']) # Convert date object to datetime

    # Prepare stock prices for joining
    df_prices_raw['date'] = pd.to_datetime(df_prices_raw['date'])

    # Join sentiment and price data
    merged_df = pd.merge(
        daily_sentiment,
        df_prices_raw[['symbol', 'date', 'close_price']],
        on=['symbol', 'date'],
        how='inner'
    )

    if merged_df.empty:
        print("No overlapping dates between sentiment and price data for correlation.")
        return

    # Calculate daily returns for price
    merged_df['daily_return'] = merged_df.groupby('symbol')['close_price'].pct_change()

    # Drop NaN values created by pct_change
    merged_df.dropna(inplace=True)

    if merged_df.empty:
        print("Not enough data after calculating daily returns for correlation.")
        return

    print("\nDaily Sentiment vs. Daily Return Correlation:")
    for symbol in merged_df['symbol'].unique():
        symbol_df = merged_df[merged_df['symbol'] == symbol]
        if len(symbol_df) > 1: # Need at least 2 data points for correlation
            correlation = symbol_df['avg_daily_sentiment'].corr(symbol_df['daily_return'])
            print(f"  {symbol}: Correlation = {correlation:.4f}")
        else:
            print(f"  {symbol}: Not enough data points for correlation.")

    return merged_df

# --- Visualization ---
def visualize_sentiment_price(df_correlated: pd.DataFrame):
    """
    Generates plots to visualize sentiment and price trends, and their correlation.
    """
    if df_correlated is None or df_correlated.empty:
        print("No correlated data available for visualization.")
        return

    print("\n--- Generating Visualizations ---")

    for symbol in df_correlated['symbol'].unique():
        symbol_df = df_correlated[df_correlated['symbol'] == symbol].sort_values('date')

        if symbol_df.empty:
            continue

        # Plot 1: Stock Price and Average Daily Sentiment Over Time
        fig, ax1 = plt.subplots(figsize=(14, 7))

        color = 'tab:blue'
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Close Price', color=color)
        ax1.plot(symbol_df['date'], symbol_df['close_price'], color=color, label='Close Price')
        ax1.tick_params(axis='y', labelcolor=color)
        ax1.tick_params(axis='x', rotation=45)

        ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis
        color = 'tab:red'
        ax2.set_ylabel('Avg Daily Sentiment', color=color)
        ax2.plot(symbol_df['date'], symbol_df['avg_daily_sentiment'], color=color, linestyle='--', label='Avg Daily Sentiment')
        ax2.tick_params(axis='y', labelcolor=color)

        fig.tight_layout() # otherwise the right y-label is slightly clipped
        plt.title(f'{symbol} Stock Price vs. Average Daily Sentiment Over Time')
        fig.legend(loc="upper left", bbox_to_anchor=(0.1,0.9))
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.show()

        # Plot 2: Scatter plot of Daily Sentiment vs. Daily Return
        plt.figure(figsize=(10, 6))
        sns.scatterplot(x='avg_daily_sentiment', y='daily_return', data=symbol_df)
        plt.title(f'{symbol} Daily Sentiment vs. Daily Return')
        plt.xlabel('Average Daily Sentiment (Compound Score)')
        plt.ylabel('Daily Stock Return (%)')
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.axhline(0, color='grey', linestyle='--', linewidth=0.8) # Zero return line
        plt.axvline(0, color='grey', linestyle='--', linewidth=0.8) # Zero sentiment line
        plt.show()

    print("Visualizations generated.")


# --- Main ETL Pipeline Execution ---
if __name__ == "__main__":
    # Check if API key is set
    if NEWS_API_KEY == "YOUR_NEWS_API_KEY":
        print("ERROR: Please replace 'YOUR_NEWS_API_KEY' with your actual NewsAPI key.")
        print("You can get one from: https://newsapi.org/")
        exit()

    print("--- Running Financial Sentiment & Price Correlation Engine ---")

    # Define date range for data extraction (NewsAPI free tier limit is 1 month)
    end_date = datetime.date.today()
    start_date = end_date - datetime.timedelta(days=29) # Max 1 month for NewsAPI free tier

    all_news_transformed_dfs = []
    all_stock_price_dfs = []

    for symbol in STOCK_SYMBOLS:
        # 1. Extract News
        # NewsAPI often works better with more general queries for broad sentiment
        # For specific stock, you might search for company name + stock symbol
        news_query = f"{symbol} stock OR {symbol.replace('GOOGL', 'Google').replace('MSFT', 'Microsoft').replace('AAPL', 'Apple')} news"
        raw_news = extract_financial_news(NEWS_API_KEY, news_query, start_date, end_date)

        # 2. Transform News
        transformed_news_df = transform_news_data(raw_news, symbol)
        if not transformed_news_df.empty:
            all_news_transformed_dfs.append(transformed_news_df)

        # 1. Extract Stock Prices
        raw_stock_df = extract_stock_prices(symbol, start_date, end_date)
        if raw_stock_df is not None and not raw_stock_df.empty:
            all_stock_price_dfs.append(raw_stock_df)

    # Concatenate all transformed news and stock data
    final_news_df = pd.concat(all_news_transformed_dfs, ignore_index=True) if all_news_transformed_dfs else pd.DataFrame()
    final_stock_df = pd.concat(all_stock_price_dfs, ignore_index=True) if all_stock_price_dfs else pd.DataFrame()

    # 3. Load Data
    load_news_sentiment_to_db(final_news_df, SessionLocal)
    load_stock_prices_to_db(final_stock_df, SessionLocal)

    # --- Analysis and Visualization ---
    correlated_data_df = perform_correlation_analysis(SessionLocal)
    visualize_sentiment_price(correlated_data_df)

    print("\nFinancial Sentiment & Price Correlation Engine pipeline completed.")
    print(f"Data stored in '{DATABASE_URL.split('///')[-1]}'.")

