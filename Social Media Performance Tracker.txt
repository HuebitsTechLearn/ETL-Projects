import pandas as pd
import datetime
import random
import uuid # For generating unique IDs

# SQLAlchemy for database interaction
from sqlalchemy import create_engine, Column, String, Integer, DateTime, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# Matplotlib and Seaborn for visualization
import matplotlib.pyplot as plt
import seaborn as sns

# --- Configuration ---
# Using SQLite for simplicity. For PostgreSQL, change this to:
# "postgresql://user:password@host:port/database_name"
# Ensure you have 'psycopg2-binary' installed for PostgreSQL.
DATABASE_URL = "sqlite:///./social_media_analytics.db"

# Social media platforms to simulate
PLATFORMS = ["Instagram", "Facebook", "YouTube"]

# Number of days to simulate data for
DAYS_TO_SIMULATE = 90 # Simulate 3 months of data

# --- Database Setup (SQLAlchemy) ---
engine = create_engine(DATABASE_URL)
Base = declarative_base()

# Define the SocialMediaMetric model
class SocialMediaMetric(Base):
    __tablename__ = "social_media_metrics"
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    platform = Column(String, nullable=False, index=True)
    snapshot_date = Column(DateTime, nullable=False, index=True) # Daily snapshot date (UTC)
    followers = Column(Integer, default=0)
    posts_count = Column(Integer, default=0)
    total_likes = Column(Integer, default=0)
    total_comments = Column(Integer, default=0)
    total_shares = Column(Integer, default=0)
    total_views = Column(Integer, default=0) # For video platforms like YouTube
    engagement_rate = Column(Float, default=0.0) # Calculated KPI
    follower_growth_rate = Column(Float, default=0.0) # Calculated KPI

    def __repr__(self):
        return (f"<SocialMediaMetric(platform='{self.platform}', date='{self.snapshot_date.strftime('%Y-%m-%d')}', "
                f"followers={self.followers}, engagement_rate={self.engagement_rate:.4f})>")

# Create tables in the database if they don't exist
Base.metadata.create_all(engine)

# Create a session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# --- Simulated Data Extraction ---
def extract_social_media_data(platforms: list, days: int):
    """
    Simulates extracting daily social media performance data for given platforms.
    In a real scenario, this would involve API calls to Meta Graph API, YouTube Data API, etc.
    """
    print(f"Simulating {days} days of social media data extraction...")
    all_simulated_data = []
    end_date = datetime.datetime.now(datetime.timezone.utc).date()
    start_date = end_date - datetime.timedelta(days=days - 1)

    for platform in platforms:
        current_followers = random.randint(1000, 10000) # Starting followers
        for i in range(days):
            current_date = start_date + datetime.timedelta(days=i)

            # Simulate daily follower growth (can be positive or negative)
            follower_change = random.randint(-50, 200)
            current_followers = max(500, current_followers + follower_change) # Ensure followers don't drop too low

            # Simulate daily post activity
            daily_posts = random.randint(0, 5)
            total_daily_likes = 0
            total_daily_comments = 0
            total_daily_shares = 0
            total_daily_views = 0 # Relevant for YouTube

            if daily_posts > 0:
                for _ in range(daily_posts):
                    # Simulate engagement per post
                    likes_per_post = random.randint(50, 500)
                    comments_per_post = random.randint(5, 50)
                    shares_per_post = random.randint(0, 20)
                    views_per_post = random.randint(1000, 10000) if platform == "YouTube" else 0

                    total_daily_likes += likes_per_post
                    total_daily_comments += comments_per_post
                    total_daily_shares += shares_per_post
                    total_daily_views += views_per_post

            all_simulated_data.append({
                'platform': platform,
                'snapshot_date': current_date,
                'followers': current_followers,
                'posts_count': daily_posts,
                'total_likes': total_daily_likes,
                'total_comments': total_daily_comments,
                'total_shares': total_daily_shares,
                'total_views': total_daily_views
            })
    df = pd.DataFrame(all_simulated_data)
    print(f"Generated {len(df)} simulated social media records.")
    return df

# --- Data Transformation ---
def transform_social_media_data(df_raw: pd.DataFrame):
    """
    Transforms raw social media metrics into actionable KPIs.
    Calculates engagement rate and follower growth rate.
    """
    print("Transforming raw data into KPIs...")
    if df_raw.empty:
        print("Raw data is empty, skipping transformation.")
        return pd.DataFrame()

    df_transformed = df_raw.copy()
    df_transformed['snapshot_date'] = pd.to_datetime(df_transformed['snapshot_date'])

    # Calculate Engagement Rate: (Likes + Comments + Shares) / (Views or a proxy for Impressions)
    # Using a simplified approach here. In real-world, you'd use actual impressions/reach.
    # For YouTube, views are a good proxy. For others, we'll use a hypothetical impression count.
    # Assume impressions are roughly 10x followers for simplicity in simulation if no views.
    df_transformed['estimated_impressions'] = df_transformed.apply(
        lambda row: row['total_views'] if row['platform'] == 'YouTube' and row['total_views'] > 0
        else row['followers'] * 10, axis=1
    )
    df_transformed['engagement_metrics_sum'] = df_transformed['total_likes'] + df_transformed['total_comments'] + df_transformed['total_shares']
    df_transformed['engagement_rate'] = df_transformed.apply(
        lambda row: (row['engagement_metrics_sum'] / row['estimated_impressions']) if row['estimated_impressions'] > 0 else 0,
        axis=1
    )
    # Cap engagement rate at 1.0 (100%) for sanity
    df_transformed['engagement_rate'] = df_transformed['engagement_rate'].clip(upper=1.0)

    # Calculate Follower Growth Rate
    df_transformed['prev_followers'] = df_transformed.groupby('platform')['followers'].shift(1)
    df_transformed['follower_growth_rate'] = df_transformed.apply(
        lambda row: (row['followers'] - row['prev_followers']) / row['prev_followers'] if row['prev_followers'] and row['prev_followers'] > 0 else 0,
        axis=1
    )
    df_transformed['follower_growth_rate'].fillna(0, inplace=True) # Fill NaN for the first day

    # Select and reorder columns for the database model
    final_df = df_transformed[[
        'platform', 'snapshot_date', 'followers', 'posts_count',
        'total_likes', 'total_comments', 'total_shares', 'total_views',
        'engagement_rate', 'follower_growth_rate'
    ]]
    print(f"Transformed {len(final_df)} KPI records.")
    return final_df

# --- Data Loading ---
def load_to_database(df: pd.DataFrame, db_session_factory):
    """
    Loads the DataFrame of social media KPIs into the database.
    """
    if df.empty:
        print("No data to load to database.")
        return

    print(f"Loading {len(df)} social media KPI records into the database...")
    with db_session_factory() as db:
        for index, row in df.iterrows():
            # Convert date to datetime object if it's not already
            snapshot_date_dt = row['snapshot_date'].to_pydatetime() if isinstance(row['snapshot_date'], pd.Timestamp) else row['snapshot_date']

            # Check for existing entry to avoid duplicates (based on platform and date)
            existing_entry = db.query(SocialMediaMetric).filter_by(
                platform=row['platform'],
                snapshot_date=snapshot_date_dt.date() # Compare only date part for daily uniqueness
            ).first()

            if not existing_entry:
                metric_entry = SocialMediaMetric(
                    platform=row['platform'],
                    snapshot_date=snapshot_date_dt,
                    followers=row['followers'],
                    posts_count=row['posts_count'],
                    total_likes=row['total_likes'],
                    total_comments=row['total_comments'],
                    total_shares=row['total_shares'],
                    total_views=row['total_views'],
                    engagement_rate=row['engagement_rate'],
                    follower_growth_rate=row['follower_growth_rate']
                )
                db.add(metric_entry)
            else:
                # Optionally, update existing entry if data is meant to be overwritten/refreshed
                existing_entry.followers = row['followers']
                existing_entry.posts_count = row['posts_count']
                existing_entry.total_likes = row['total_likes']
                existing_entry.total_comments = row['total_comments']
                existing_entry.total_shares = row['total_shares']
                existing_entry.total_views = row['total_views']
                existing_entry.engagement_rate = row['engagement_rate']
                existing_entry.follower_growth_rate = row['follower_growth_rate']
        db.commit()
    print("Social media KPIs loaded successfully.")

    # --- Alternative Loading Options (Comments) ---
    # For PostgreSQL:
    # Change DATABASE_URL to "postgresql://user:password@host:port/database_name"
    # Ensure 'psycopg2-binary' is installed: pip install psycopg2-binary
    # The SQLAlchemy code remains largely the same.

    # For Google Sheets:
    # This would require Google API credentials (service account or OAuth client ID)
    # and libraries like 'gspread' or 'google-api-python-client'.
    # Example (conceptual):
    # import gspread
    # gc = gspread.service_account(filename='path/to/your/credentials.json')
    # spreadsheet = gc.open("Your Social Media Data Sheet")
    # worksheet = spreadsheet.worksheet("Daily Metrics")
    # worksheet.append_rows(df.values.tolist()) # Append DataFrame rows to sheet
    # print("Data loaded to Google Sheet.")


# --- Visualization ---
def visualize_performance(db_session_factory):
    """
    Fetches aggregated social media data from the database and visualizes KPIs.
    """
    print("\n--- Generating Performance Visualizations ---")

    with db_session_factory() as db:
        # Fetch all historical social media metrics
        historical_records = db.query(SocialMediaMetric).all()
        if not historical_records:
            print("No historical social media data found for visualization.")
            return

        # Convert to DataFrame
        data = [{
            'platform': rec.platform,
            'snapshot_date': rec.snapshot_date,
            'followers': rec.followers,
            'engagement_rate': rec.engagement_rate,
            'follower_growth_rate': rec.follower_growth_rate
        } for rec in historical_records]
        df_historical = pd.DataFrame(data)

    if df_historical.empty:
        print("No data to visualize.")
        return

    df_historical['snapshot_date'] = pd.to_datetime(df_historical['snapshot_date'])
    df_historical.set_index('snapshot_date', inplace=True)

    # Plot 1: Follower Growth Over Time by Platform
    plt.figure(figsize=(14, 7))
    sns.lineplot(data=df_historical, x=df_historical.index, y='followers', hue='platform', marker='o', errorbar=None)
    plt.title('Follower Growth Over Time by Platform')
    plt.xlabel('Date (UTC)')
    plt.ylabel('Followers')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Plot 2: Average Daily Engagement Rate Over Time by Platform
    plt.figure(figsize=(14, 7))
    sns.lineplot(data=df_historical, x=df_historical.index, y='engagement_rate', hue='platform', marker='o', errorbar=None)
    plt.title('Average Daily Engagement Rate Over Time by Platform')
    plt.xlabel('Date (UTC)')
    plt.ylabel('Engagement Rate')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Plot 3: Follower Growth Rate (Percentage Change)
    plt.figure(figsize=(14, 7))
    sns.lineplot(data=df_historical, x=df_historical.index, y='follower_growth_rate', hue='platform', marker='o', errorbar=None)
    plt.title('Daily Follower Growth Rate Over Time by Platform')
    plt.xlabel('Date (UTC)')
    plt.ylabel('Follower Growth Rate (Percentage)')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    print("Performance visualizations generated.")


# --- Main ETL Pipeline Execution ---
if __name__ == "__main__":
    print("--- Running Social Media Performance Tracker Pipeline ---")

    # 1. Simulate Data Extraction
    raw_social_data_df = extract_social_media_data(PLATFORMS, DAYS_TO_SIMULATE)

    # 2. Transform Data
    transformed_kpis_df = transform_social_media_data(raw_social_data_df)

    # 3. Load Data
    load_to_database(transformed_kpis_df, SessionLocal)

    # --- Analysis and Visualization ---
    visualize_performance(SessionLocal)

    print("\nSocial Media Performance Tracker pipeline completed.")
    print(f"Data stored in '{DATABASE_URL.split('///')[-1]}'.")

